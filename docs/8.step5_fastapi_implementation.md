# Step 5: FastAPI Implementation - First Endpoint

**Objective:** Implement the complete search endpoint with proper request handling, integration with SearXNG, and AI reasoning.

This step will guide you through building a production-ready FastAPI endpoint that orchestrates the entire search pipeline.

---

## üéØ What You Will Learn

- How FastAPI works with routing and dependencies
- Request/response validation with Pydantic
- Async programming for high performance
- Integration with external APIs (SearXNG)
- Error handling and HTTP exceptions
- Logging and monitoring
- Building the complete search pipeline
- Testing the endpoint

---

## üèóÔ∏è Architecture of the Search Endpoint

```
User Request (JSON)
       ‚Üì
FastAPI Route Handler
       ‚Üì
Validate Request (Pydantic)
       ‚Üì
Search with SearXNG
       ‚Üì
Process Results
       ‚Üì
AI Reasoning (DSPy)
       ‚Üì
Format Output (JSON + Markdown)
       ‚Üì
Return Response
```

---

## üõ†Ô∏è Step-by-Step Implementation

### Step 5.1: Update Request/Response Schemas

Update `app/schemas/search.py`:

```python
"""
Pydantic Data Models for Search API

This module defines request/response schemas with validation.
"""

from pydantic import BaseModel, Field
from typing import Optional, List
from datetime import datetime


class SearchRequestSchema(BaseModel):
    """
    Schema for search API request

    Example:
        {
            "query": "Best FastAPI practices",
            "limit": 10,
            "language": "en"
        }
    """
    query: str = Field(
        ...,
        min_length=1,
        max_length=500,
        description="Search query string"
    )
    limit: Optional[int] = Field(
        10,
        ge=1,
        le=50,
        description="Maximum number of results (1-50)"
    )
    language: Optional[str] = Field(
        "en",
        description="Language code (e.g., 'en', 'fr')"
    )

    class Config:
        json_schema_extra = {
            "example": {
                "query": "Best FastAPI practices",
                "limit": 10,
                "language": "en"
            }
        }


class SearchResultSchema(BaseModel):
    """Schema for individual search result"""

    title: str = Field(..., description="Result title")
    url: str = Field(..., description="Result URL")
    snippet: str = Field(..., description="Result snippet/summary")
    source: Optional[str] = Field(None, description="Search engine source")

    class Config:
        json_schema_extra = {
            "example": {
                "title": "FastAPI Best Practices",
                "url": "https://example.com/fastapi-practices",
                "snippet": "Learn the best practices for building FastAPI applications...",
                "source": "Google"
            }
        }


class AIInsightSchema(BaseModel):
    """Schema for AI insights"""

    key_points: List[str] = Field(..., description="Main insights extracted")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confidence score")
    summary: str = Field(..., description="AI-generated summary")


class SearchResponseSchema(BaseModel):
    """Complete response schema"""

    query: str = Field(..., description="Original query")
    results_count: int = Field(..., description="Number of results")
    results: List[SearchResultSchema] = Field(..., description="Search results")

    # AI processed output
    markdown_summary: str = Field(..., description="Human-readable markdown")
    json_data: dict = Field(..., description="Structured JSON data")
    insights: Optional[AIInsightSchema] = Field(None, description="AI insights")

    # Metadata
    processing_time: float = Field(..., description="Time to process (seconds)")
    timestamp: datetime = Field(default_factory=datetime.utcnow)

    class Config:
        json_schema_extra = {
            "example": {
                "query": "Best FastAPI practices",
                "results_count": 10,
                "results": [
                    {
                        "title": "FastAPI Documentation",
                        "url": "https://fastapi.tiangolo.com",
                        "snippet": "Official FastAPI documentation...",
                        "source": "Google"
                    }
                ],
                "markdown_summary": "# FastAPI Best Practices\n\n## Key Points\n1. Use async endpoints...",
                "json_data": {
                    "practices": ["Use async", "Validate inputs", "Use dependencies"]
                },
                "insights": {
                    "key_points": ["Use async endpoints", "Proper validation"],
                    "confidence": 0.95,
                    "summary": "FastAPI follows async-first design..."
                },
                "processing_time": 2.5,
                "timestamp": "2024-01-10T10:30:00"
            }
        }
```

---

### Step 5.2: Implement SearXNG Client

Update `app/search/searxng_client.py`:

```python
"""
SearXNG Search Engine Client

This module handles communication with SearXNG for live web search.
"""

import httpx
import logging
from typing import List, Dict, Optional
from app.core.config import settings

logger = logging.getLogger(__name__)


class SearXNGClient:
    """Client for interacting with SearXNG search engine"""

    def __init__(self, base_url: Optional[str] = None):
        """
        Initialize SearXNG client

        Args:
            base_url: SearXNG instance URL (defaults to settings)
        """
        self.base_url = base_url or settings.SEARXNG_URL
        self.client = httpx.AsyncClient(timeout=30.0)

    async def search(
        self,
        query: str,
        limit: int = 10,
        language: str = "en"
    ) -> List[Dict]:
        """
        Perform search query

        Args:
            query: Search query string
            limit: Maximum number of results
            language: Language code

        Returns:
            List of search results

        Raises:
            httpx.HTTPError: If search request fails
        """
        try:
            logger.info(f"Searching SearXNG for: {query}")

            # Prepare request
            params = {
                "q": query,
                "format": "json",
                "limit": limit,
                "language": language,
            }

            # Make request
            response = await self.client.get(
                f"{self.base_url}/search",
                params=params
            )
            response.raise_for_status()

            # Parse response
            data = response.json()
            results = []

            for result in data.get("results", []):
                results.append({
                    "title": result.get("title", ""),
                    "url": result.get("url", ""),
                    "snippet": result.get("content", ""),
                    "source": result.get("engine", "Unknown"),
                })

            logger.info(f"Found {len(results)} results")
            return results

        except httpx.HTTPError as e:
            logger.error(f"SearXNG request failed: {e}")
            raise
        except Exception as e:
            logger.error(f"Error parsing SearXNG response: {e}")
            raise

    async def close(self):
        """Close HTTP client"""
        await self.client.aclose()

    async def __aenter__(self):
        """Async context manager entry"""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self.close()
```

---

### Step 5.3: Implement AI Reasoning Pipeline

Update `app/ai/dspy_pipeline.py`:

```python
"""
DSPy AI Reasoning Pipeline

This module uses DSPy for AI-based reasoning and structuring of search results.
"""

import logging
from typing import List, Dict
import json

logger = logging.getLogger(__name__)


class DSPyPipeline:
    """
    AI reasoning pipeline for processing and structuring search results.

    Note: This is a placeholder implementation. In production, integrate:
    - dspy-ai for advanced reasoning
    - GPT/Claude for high-quality processing
    - Caching for repeated queries
    """

    def __init__(self):
        """Initialize DSPy pipeline"""
        self.model = "mock"  # Placeholder

    async def process_results(
        self,
        query: str,
        results: List[Dict]
    ) -> Dict:
        """
        Process search results with AI reasoning

        Args:
            query: Original search query
            results: Raw search results from SearXNG

        Returns:
            Structured data with insights
        """
        logger.info(f"Processing {len(results)} results with AI")

        try:
            # Extract key information
            key_points = self._extract_key_points(query, results)

            # Generate markdown summary
            markdown = self._generate_markdown(query, results, key_points)

            # Generate structured JSON
            json_data = self._generate_json(query, results, key_points)

            return {
                "key_points": key_points,
                "markdown_summary": markdown,
                "json_data": json_data,
                "confidence": 0.85,  # Mock confidence
            }

        except Exception as e:
            logger.error(f"Error in AI processing: {e}")
            raise

    def _extract_key_points(self, query: str, results: List[Dict]) -> List[str]:
        """Extract key points from search results"""
        key_points = []

        # Mock implementation - in production use DSPy
        for i, result in enumerate(results[:5], 1):
            title = result.get("title", "")
            if title:
                key_points.append(f"{i}. {title}")

        return key_points if key_points else ["No clear insights found"]

    def _generate_markdown(
        self,
        query: str,
        results: List[Dict],
        key_points: List[str]
    ) -> str:
        """Generate markdown summary of results"""
        markdown_parts = [
            f"# Search Results for: {query}",
            "",
            "## Key Points",
            ""
        ]

        # Add key points
        for point in key_points:
            markdown_parts.append(f"- {point}")

        markdown_parts.extend(["", "## Top Results", ""])

        # Add top results with links
        for result in results[:5]:
            title = result.get("title", "")
            url = result.get("url", "")
            snippet = result.get("snippet", "")

            markdown_parts.append(f"### [{title}]({url})")
            markdown_parts.append(f"{snippet[:200]}...")
            markdown_parts.append("")

        return "\n".join(markdown_parts)

    def _generate_json(
        self,
        query: str,
        results: List[Dict],
        key_points: List[str]
    ) -> Dict:
        """Generate structured JSON output"""
        return {
            "query": query,
            "key_insights": key_points,
            "results_summary": {
                "total": len(results),
                "top_3": [
                    {
                        "title": r.get("title"),
                        "url": r.get("url"),
                        "snippet": r.get("snippet", "")[:150]
                    }
                    for r in results[:3]
                ]
            },
            "metadata": {
                "model": self.model,
                "confidence": 0.85,
            }
        }
```

---

### Step 5.4: Implement API Router

Update `app/api/search.py`:

```python
"""
Search API Routes

This module defines the search endpoints.
"""

import logging
import time
from fastapi import APIRouter, HTTPException, Depends
from datetime import datetime

from app.schemas.search import (
    SearchRequestSchema,
    SearchResponseSchema,
    SearchResultSchema,
)
from app.search.searxng_client import SearXNGClient
from app.ai.dspy_pipeline import DSPyPipeline
from app.core.config import settings

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/api/v1",
    tags=["search"],
    responses={
        400: {"description": "Invalid request"},
        500: {"description": "Internal server error"},
    }
)


# Dependency injections
async def get_searxng_client() -> SearXNGClient:
    """Dependency injection for SearXNG client"""
    return SearXNGClient()


async def get_dspy_pipeline() -> DSPyPipeline:
    """Dependency injection for DSPy pipeline"""
    return DSPyPipeline()


@router.post(
    "/search",
    response_model=SearchResponseSchema,
    summary="Execute AI-powered search",
    description="Search the web and get structured results with AI insights"
)
async def search(
    request: SearchRequestSchema,
    searxng_client: SearXNGClient = Depends(get_searxng_client),
    dspy_pipeline: DSPyPipeline = Depends(get_dspy_pipeline),
) -> SearchResponseSchema:
    """
    Execute a search query and return structured results

    This endpoint:
    1. Validates the search query
    2. Searches the web using SearXNG
    3. Processes results with AI
    4. Returns structured JSON + Markdown

    Args:
        request: Search request with query and parameters
        searxng_client: SearXNG client (injected)
        dspy_pipeline: AI pipeline (injected)

    Returns:
        SearchResponseSchema with results and AI insights

    Raises:
        HTTPException: 400 if query is invalid, 500 if search fails
    """
    start_time = time.time()

    try:
        logger.info(f"Processing search request: {request.query}")

        # Validate query
        query = request.query.strip()
        if not query:
            raise HTTPException(
                status_code=400,
                detail="Query cannot be empty"
            )

        if len(query) > 500:
            raise HTTPException(
                status_code=400,
                detail="Query too long (max 500 characters)"
            )

        # Perform search
        logger.info(f"Querying SearXNG with: {query}")
        raw_results = await searxng_client.search(
            query=query,
            limit=request.limit,
            language=request.language
        )

        if not raw_results:
            logger.warning(f"No results found for: {query}")

        # Process results with AI
        logger.info("Processing results with AI")
        ai_output = await dspy_pipeline.process_results(query, raw_results)

        # Convert raw results to schema
        results = [
            SearchResultSchema(**result)
            for result in raw_results
        ]

        # Calculate processing time
        processing_time = time.time() - start_time

        # Build response
        response = SearchResponseSchema(
            query=query,
            results_count=len(results),
            results=results,
            markdown_summary=ai_output["markdown_summary"],
            json_data=ai_output["json_data"],
            insights={
                "key_points": ai_output["key_points"],
                "confidence": ai_output["confidence"],
                "summary": f"Found {len(results)} relevant results for '{query}'"
            },
            processing_time=processing_time,
        )

        logger.info(f"Search completed in {processing_time:.2f}s")
        return response

    except HTTPException:
        raise
    except ConnectionError as e:
        logger.error(f"Connection error with SearXNG: {e}")
        raise HTTPException(
            status_code=503,
            detail="Search engine unavailable"
        )
    except Exception as e:
        logger.error(f"Unexpected error in search: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="Internal server error during search"
        )


@router.get(
    "/search/health",
    summary="Check search service health",
)
async def search_health(
    searxng_client: SearXNGClient = Depends(get_searxng_client),
) -> dict:
    """
    Health check for search service

    Returns:
        Status of search dependencies
    """
    try:
        logger.info("Performing health check")

        # Check SearXNG connectivity
        health_status = {
            "status": "healthy",
            "searxng": "unknown",
            "ai": "ready",
            "timestamp": datetime.utcnow().isoformat(),
        }

        # Try simple search to verify SearXNG
        try:
            results = await searxng_client.search("test", limit=1)
            health_status["searxng"] = "healthy" if results else "no_results"
        except Exception as e:
            logger.warning(f"SearXNG health check failed: {e}")
            health_status["searxng"] = "unhealthy"
            health_status["status"] = "degraded"

        return health_status

    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(
            status_code=503,
            detail="Health check failed"
        )
```

---

### Step 5.5: Update Main Application

Update `app/main.py`:

```python
"""
FastAPI Application Entry Point

This is the main entry point for the SearchFlow application.
It initializes FastAPI, configures middleware, and mounts all routes.
"""

import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

from app.core.config import settings
from app.core.logging import setup_logging
from app.api import search

# Setup logging
logger = setup_logging()

# Create FastAPI app
app = FastAPI(
    title="SearchFlow API",
    description="AI-powered search backend that returns structured knowledge",
    version="0.1.0",
    docs_url="/docs",
    redoc_url="/redoc",
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """Handle uncaught exceptions"""
    logger.error(f"Uncaught exception: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error"}
    )


# Health endpoints
@app.get("/", tags=["health"])
async def root():
    """Root endpoint - service status"""
    return {
        "message": "SearchFlow is running",
        "version": "0.1.0",
        "docs": "/docs"
    }


@app.get("/health", tags=["health"])
async def health():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "version": "0.1.0",
        "service": "SearchFlow API"
    }


# Include search router
app.include_router(search.router)


# Startup event
@app.on_event("startup")
async def startup_event():
    """Execute on application startup"""
    logger.info("SearchFlow API starting up")
    logger.info(f"Environment: Debug={settings.DEBUG}")
    logger.info(f"SearXNG URL: {settings.SEARXNG_URL}")


# Shutdown event
@app.on_event("shutdown")
async def shutdown_event():
    """Execute on application shutdown"""
    logger.info("SearchFlow API shutting down")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level=settings.LOG_LEVEL.lower()
    )
```

---

### Step 5.6: Update Test Files

Update `tests/test_search_api.py`:

```python
"""
Search API Tests

Tests for search endpoint functionality.
"""

import pytest
from fastapi.testclient import TestClient
from app.main import app
from app.schemas.search import SearchRequestSchema

client = TestClient(app)


@pytest.mark.unit
class TestSearchEndpoint:
    """Tests for search endpoint"""

    def test_search_endpoint_exists(self):
        """Test search endpoint is accessible"""
        response = client.get("/docs")
        assert response.status_code == 200
        assert "search" in response.text.lower()

    def test_search_with_valid_query(self):
        """Test search with valid query"""
        payload = {
            "query": "FastAPI",
            "limit": 5,
            "language": "en"
        }
        response = client.post("/api/v1/search", json=payload)

        # Should either succeed or fail with proper error
        assert response.status_code in [200, 503]

        if response.status_code == 200:
            data = response.json()
            assert "query" in data
            assert data["query"] == "FastAPI"
            assert "results_count" in data
            assert "markdown_summary" in data
            assert "json_data" in data

    def test_search_with_empty_query(self):
        """Test search rejects empty query"""
        payload = {
            "query": "",
            "limit": 5
        }
        response = client.post("/api/v1/search", json=payload)
        assert response.status_code == 422  # Validation error

    def test_search_with_limit_validation(self):
        """Test search validates limit parameter"""
        payload = {
            "query": "test",
            "limit": 100  # Over max of 50
        }
        response = client.post("/api/v1/search", json=payload)
        assert response.status_code == 422

    def test_search_response_structure(self):
        """Test search response has correct structure"""
        payload = {
            "query": "python",
            "limit": 3
        }
        response = client.post("/api/v1/search", json=payload)

        if response.status_code == 200:
            data = response.json()

            # Check required fields
            required_fields = [
                "query", "results_count", "results",
                "markdown_summary", "json_data",
                "processing_time", "timestamp"
            ]
            for field in required_fields:
                assert field in data, f"Missing field: {field}"

            # Check types
            assert isinstance(data["query"], str)
            assert isinstance(data["results_count"], int)
            assert isinstance(data["results"], list)
            assert isinstance(data["markdown_summary"], str)
            assert isinstance(data["processing_time"], float)


@pytest.mark.unit
def test_search_health_endpoint():
    """Test search health endpoint"""
    response = client.get("/api/v1/search/health")

    # Health check may be degraded if SearXNG unavailable
    assert response.status_code in [200, 503]

    if response.status_code == 200:
        data = response.json()
        assert "status" in data
        assert data["status"] in ["healthy", "degraded"]
```

---

### Step 5.7: Create Integration Test

Create `tests/test_e2e.py`:

```python
"""
End-to-End Integration Tests

Tests for complete workflow including SearXNG integration.
"""

import pytest
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)


@pytest.mark.integration
class TestEndToEnd:
    """End-to-end integration tests"""

    @pytest.mark.slow
    def test_complete_search_workflow(self):
        """Test complete search workflow"""
        payload = {
            "query": "how to learn python programming",
            "limit": 5,
            "language": "en"
        }

        response = client.post("/api/v1/search", json=payload)

        # May be 503 if SearXNG unavailable in test environment
        if response.status_code == 200:
            data = response.json()

            # Verify complete workflow
            assert data["results_count"] > 0
            assert len(data["results"]) <= 5
            assert len(data["markdown_summary"]) > 0
            assert len(data["json_data"]) > 0
            assert data["processing_time"] > 0

    def test_api_returns_proper_errors(self):
        """Test API returns proper error messages"""
        # Invalid query (too long)
        payload = {
            "query": "a" * 1000,
            "limit": 5
        }
        response = client.post("/api/v1/search", json=payload)
        assert response.status_code == 422

        # Invalid limit
        payload = {
            "query": "test",
            "limit": -1
        }
        response = client.post("/api/v1/search", json=payload)
        assert response.status_code == 422
```

---

### Step 5.8: Run the Application

**Start with Docker Compose:**

```bash
# From project root
docker-compose up
```

**Or run locally:**

```bash
# Activate venv
source .venv/bin/activate

# Make sure SearXNG is running (optional)
# For testing without SearXNG, mock responses will be used

# Run application
python app/main.py
```

**Access endpoints:**

- API: http://localhost:8000
- Docs: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc
- Health: http://localhost:8000/health

---

### Step 5.9: Test the Endpoint

**Using curl:**

```bash
curl -X POST http://localhost:8000/api/v1/search \
  -H "Content-Type: application/json" \
  -d '{"query": "FastAPI best practices", "limit": 5}'
```

**Using Python:**

```python
import httpx

async def test_search():
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8000/api/v1/search",
            json={
                "query": "FastAPI",
                "limit": 5
            }
        )
        print(response.json())

# Run
import asyncio
asyncio.run(test_search())
```

**Using FastAPI Docs:**

1. Go to http://localhost:8000/docs
2. Click on POST /api/v1/search
3. Click "Try it out"
4. Modify the request body
5. Click "Execute"

---

### Step 5.10: Run Tests

**Run all tests:**

```bash
pytest tests/ -v
```

**Run specific test file:**

```bash
pytest tests/test_search_api.py -v
```

**Run with coverage:**

```bash
pytest tests/ -v --cov=app --cov-report=html
```

**Run only integration tests:**

```bash
pytest tests/test_e2e.py -v -m integration
```

---

## üèÜ What You've Built

### Complete Search Pipeline

1. **Request Validation** - Pydantic schemas ensure data quality
2. **Search Integration** - SearXNG client handles web search
3. **AI Processing** - DSPy pipeline structures results
4. **Response Generation** - JSON + Markdown outputs
5. **Error Handling** - Proper HTTP exceptions and logging
6. **Testing** - Unit and integration tests
7. **Documentation** - Auto-generated Swagger/ReDoc docs

### Key Features

- ‚úÖ Natural language queries
- ‚úÖ Live web search
- ‚úÖ AI reasoning
- ‚úÖ Dual output (JSON + Markdown)
- ‚úÖ Error handling
- ‚úÖ Logging and monitoring
- ‚úÖ Health checks
- ‚úÖ Dependency injection
- ‚úÖ Async/await for performance
- ‚úÖ Comprehensive testing

---

## üìã Example Responses

**Request:**

```json
{
  "query": "What is machine learning?",
  "limit": 5
}
```

**Response:**

```json
{
  "query": "What is machine learning?",
  "results_count": 5,
  "results": [
    {
      "title": "Machine Learning - Wikipedia",
      "url": "https://en.wikipedia.org/wiki/Machine_learning",
      "snippet": "Machine learning is a subset of artificial intelligence...",
      "source": "Google"
    }
  ],
  "markdown_summary": "# Search Results for: What is machine learning?\n\n## Key Points\n1. Machine Learning - Wikipedia\n2. Introduction to Machine Learning\n...",
  "json_data": {
    "query": "What is machine learning?",
    "key_insights": ["Machine Learning - Wikipedia", "..."],
    "results_summary": {
      "total": 5,
      "top_3": [...]
    }
  },
  "insights": {
    "key_points": ["Machine Learning - Wikipedia", "..."],
    "confidence": 0.85,
    "summary": "Found 5 relevant results..."
  },
  "processing_time": 2.34,
  "timestamp": "2024-01-10T10:30:00"
}
```

---

## ‚úÖ Verification Checklist

After completing Step 5:

- [ ] `app/api/search.py` has complete implementation
- [ ] `app/schemas/search.py` has all schemas
- [ ] `app/search/searxng_client.py` has SearXNG integration
- [ ] `app/ai/dspy_pipeline.py` has AI processing
- [ ] `app/main.py` includes search router
- [ ] All test files pass
- [ ] API docs are accessible at `/docs`
- [ ] Endpoint returns proper responses
- [ ] Error handling works correctly
- [ ] Logging shows all operations

---

## üéì Key Concepts Learned

1. **FastAPI Routing** - Creating endpoints and routes
2. **Dependency Injection** - Using Depends for DI
3. **Async Programming** - Async/await for performance
4. **Request Validation** - Pydantic for schema validation
5. **HTTP Exceptions** - Proper error responses
6. **Logging** - Structured logging throughout
7. **Client Integration** - Calling external APIs
8. **Pipeline Architecture** - Building processing pipelines
9. **Error Handling** - Graceful failure management
10. **API Documentation** - Auto-generated docs

---

## üîó Next: Production Deployment

After completing all 5 steps, your SearchFlow is ready for:

1. **Local Development** - Run and test locally
2. **Docker Deployment** - Deploy as containers
3. **CI/CD Pipeline** - Automated testing and deployment
4. **Production** - Scale and monitor

---

## üí° Future Enhancements

### Phase 2: Advanced Features

- Query decomposition for complex questions
- Multi-language support
- Result caching
- User authentication
- Rate limiting

### Phase 3: AI Enhancement

- Integrate real DSPy
- Use GPT-4/Claude
- Vector embeddings
- Knowledge graphs
- Entity extraction

### Phase 4: Operations

- Monitoring and alerts
- Performance optimization
- Database integration
- Analytics dashboards
- Multi-region deployment

---
